[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m -14189.3         [30mT:[39m 0:00:00
Traceback (most recent call last):
  File "train.py", line 109, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 93, in train
    common_metrics['episode_reward'] = evaluate(env, agent, cfg.eval_episodes, step, eval_numbering)
  File "train.py", line 34, in evaluate
    action = agent.plan(s, eval_mode=True, step=step, t0=t==0)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 106, in plan
    pi_actions[t] = self.model.pi(s, self.cfg.min_std) #samples an action and save
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 34, in pi
    mu = torch.tanh(self._pi(s))
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt