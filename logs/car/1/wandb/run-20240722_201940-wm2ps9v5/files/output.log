[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m -5.7             [30mT:[39m 0:00:00
Animation saved successfully
 [32meval[39m    [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 13.9             [30mT:[39m 0:00:00
 [34mtrain[39m   [30mE:[39m 2                [30mS:[39m 1,000            [30mR:[39m -8.7             [30mT:[39m 0:01:06
 [34mtrain[39m   [30mE:[39m 3                [30mS:[39m 2,000            [30mR:[39m 41.3             [30mT:[39m 0:01:06
 [34mtrain[39m   [30mE:[39m 4                [30mS:[39m 3,000            [30mR:[39m 39.8             [30mT:[39m 0:01:06
 [34mtrain[39m   [30mE:[39m 5                [30mS:[39m 4,000            [30mR:[39m -8.1             [30mT:[39m 0:01:06
 [34mtrain[39m   [30mE:[39m 6                [30mS:[39m 5,000            [30mR:[39m 50.8             [30mT:[39m 0:01:07
 [34mtrain[39m   [30mE:[39m 7                [30mS:[39m 6,000            [30mR:[39m 46.7             [30mT:[39m 0:01:07
 [34mtrain[39m   [30mE:[39m 8                [30mS:[39m 7,000            [30mR:[39m 36.1             [30mT:[39m 0:01:07
 [34mtrain[39m   [30mE:[39m 9                [30mS:[39m 8,000            [30mR:[39m 14.0             [30mT:[39m 0:01:08
 [34mtrain[39m   [30mE:[39m 10               [30mS:[39m 9,000            [30mR:[39m 47.1             [30mT:[39m 0:01:08
Traceback (most recent call last):
  File "train.py", line 146, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 98, in train
    action = agent.plan(s,step=step,t0=episode.first)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 130, in plan
    value = self.estimate_value(s, actions, horizon).nan_to_num_(0) #추론된 reward 모델을 통해 reward계산
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 83, in estimate_value
    G += discount * torch.min(*self.model.Q(s, self.model.pi(s, self.cfg.min_std))) #terminal reward, s와 action 값을 넣고 value를 계산
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 38, in pi
    return h.TruncatedNormal(mu, std).sample(clip=0.3) #샘플링된 값을 -0.3에서 0.3으로 제한
  File "/home/jeongtae/RLMPPI/src/algorithm/helper.py", line 76, in sample
    eps = torch.clamp(eps, -clip, clip)
KeyboardInterrupt