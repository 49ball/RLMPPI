[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 150.7            [30mT:[39m 0:00:00
 [32meval[39m    [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 1805.7           [30mT:[39m 0:00:00
 [34mtrain[39m   [30mE:[39m 2                [30mS:[39m 1,000            [30mR:[39m 5698.5           [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 3                [30mS:[39m 2,000            [30mR:[39m -21.4            [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 4                [30mS:[39m 3,000            [30mR:[39m 21.5             [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 5                [30mS:[39m 4,000            [30mR:[39m -4.2             [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 6                [30mS:[39m 5,000            [30mR:[39m -30.0            [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 7                [30mS:[39m 6,000            [30mR:[39m 6150.5           [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 8                [30mS:[39m 7,000            [30mR:[39m 107.8            [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 9                [30mS:[39m 8,000            [30mR:[39m -30.0            [30mT:[39m 0:00:05
 [34mtrain[39m   [30mE:[39m 10               [30mS:[39m 9,000            [30mR:[39m 21.5             [30mT:[39m 0:00:05
> /home/jeongtae/RLMPPI/src/algorithm/helper.py(255)sample()
-> if invalid_mask.any():
tensor([ True,  True, False, False,  True,  True,  True,  True, False, False,
        False,  True,  True, False,  True,  True, False,  True,  True,  True,
         True,  True,  True, False, False,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True, False,  True, False,  True,  True,
         True,  True,  True,  True, False,  True,  True,  True,  True,  True,
        False, False, False,  True,  True,  True,  True,  True,  True,  True,
        False,  True, False,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True, False,  True,  True,  True,
        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True, False,  True, False,  True,  True,  True, False,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True, False,  True, False,  True,  True,
        False, False,  True,  True,  True, False,  True,  True,  True,  True,
         True,  True,  True,  True, False,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True, False,  True,  True,  True, False,
         True, False,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True, False,  True,  True,
         True,  True,  True,  True,  True, False,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True, False,  True,  True, False,
         True,  True, False,  True, False,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True, False,  True, False,  True,
         True,  True, False, False,  True, False,  True, False, False,  True,
        False,  True,  True, False, False, False,  True,  True,  True,  True,
         True,  True,  True,  True,  True, False,  True, False,  True,  True,
         True, False,  True,  True, False, False,  True,  True,  True,  True,
         True,  True,  True, False,  True,  True,  True, False, False,  True,
         True,  True,  True, False,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True, False, False,
         True,  True,  True,  True, False,  True,  True,  True, False,  True,
         True, False, False,  True,  True,  True, False,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
         True, False,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True, False, False,  True,
         True, False,  True, False,  True,  True,  True,  True,  True,  True,
         True, False,  True,  True, False, False,  True,  True,  True, False,
         True,  True, False,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True, False,  True,  True, False,  True, False,
         True, False,  True,  True,  True,  True,  True,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
        False,  True,  True, False,  True, False, False,  True,  True,  True,
         True,  True,  True, False,  True,  True, False,  True,  True,  True,
         True, False,  True,  True,  True,  True,  True, False,  True,  True,
         True,  True,  True,  True, False,  True,  True, False,  True, False,
         True,  True, False,  True,  True,  True,  True,  True,  True,  True,
         True, False,  True,  True,  True,  True,  True, False,  True,  True,
         True,  True,  True, False,  True,  True,  True,  True,  True,  True,
         True,  True, False,  True,  True,  True, False,  True,  True,  True,
         True,  True, False,  True, False,  True, False,  True,  True,  True,
         True,  True,  True,  True,  True,  True,  True,  True, False,  True,
         True,  True], device='cuda:0')
tensor([4718, 3686, 3464,  678, 2584, 5409, 5207, 3546,  821, 3514, 3106, 2574,
        2340, 3284, 4667, 4125, 3179, 5082,  341, 5434,  279, 1093, 2312,  587,
        3412,  253, 1562,  322, 3854, 3669, 2078, 1033, 4104, 1857, 4359, 2676,
        2875,  150, 3541, 1911, 1250, 2370, 2099, 2580, 5371, 2003, 4248, 3038,
        4874, 1934, 1336, 5010,  229, 5219, 3057, 2067, 5453,  308, 2828,  164,
        3135,  972, 3451, 5379, 4799, 2465, 3880, 4265, 2712, 2890,  816, 2030,
         770, 3977, 2618, 2450, 4858, 2885, 2239, 1459,  380, 2287,  135, 1597,
        2758, 5300,  597, 3693, 2739, 4263,  778,  439, 2187, 4367, 1052, 4213,
        1592, 1185,   88, 2184, 2092, 3621,  374,  825,   87,  620, 3581, 2206,
        1755, 3065, 5445, 4586, 3834, 5048,  209,  371, 2602, 1903, 5148, 2684,
        2949, 4906, 2438, 4810, 1378, 1505, 1793, 3011, 1202, 3685,  773,  516,
        4774, 1290, 2118, 3137, 2876,  401, 4795, 5221, 4450, 1558, 2887, 1852,
        3048, 5344, 1705, 3671, 1781, 4249, 1780, 4878, 4129, 4186, 2578, 1152,
         218, 1759,  195, 3803, 3679, 2352, 4213, 2930,  210,  731, 1061, 1832,
         276, 3315, 2803, 3380, 2362, 4655, 2477,   81, 4789, 3604, 4525, 5225,
         268, 1281,  334, 2306, 4741,  430, 2598,  682, 4240, 4622,  228, 2668,
        1304, 5229, 5183, 3361, 5340, 1880, 4920, 2374, 1285, 5167, 3752,  342,
        4775, 3843, 3314, 4026, 1378, 3290, 4461,  285,  709, 4628, 3384, 2905,
        1350, 1618, 4787, 2306,  340, 4916, 1113, 4542, 4835, 2670, 3279, 2883,
        3419, 4694, 1549, 4846, 3117,  627, 1238, 3266, 1304,  713,  874, 4640,
        3299, 5287, 1885, 3264, 3282, 3370,  312, 4121, 5206, 2923, 1056, 4135,
          38, 1793, 5044, 3226, 4695, 3312, 4523, 4823, 1754,  669, 3948, 2405,
         689, 3234,  190, 1096, 4322,   63, 1661,  112, 5466, 3193, 1609, 5104,
        2777, 2500, 3223, 1434, 1671, 2042, 1330, 3206, 3814,  379, 5327, 4136,
        4416, 4128,  422, 2643, 2433, 3690, 2449, 3858, 3739, 3820, 3386,  817,
        4177, 4283, 4953, 1274,  952, 2145, 1752, 4465, 3359, 4172, 2338,  545,
         627, 2051, 1063, 4513, 3286, 3777, 2693,  465, 1281, 2537, 1088,  134,
        4011, 3967, 1804, 4633, 2342, 4745, 3609, 3187, 1099, 2896, 4902, 1670,
        4778, 4978, 1801, 3746, 4927,  399, 4818, 1047, 4666, 3662, 1878,  819,
        3474, 4656, 4519, 3437, 5249, 3234, 1084, 2346, 1838, 5437, 2087, 5441,
        2839,  929,  393, 2035,  671, 3470, 2265, 5432, 5110,  807, 2164, 2537,
        3081, 4280, 2672, 2297, 1423, 5009,  117, 4515, 1035, 3760, 1935, 5343,
         668, 1030, 4876, 3200, 1692,    4, 1241,  787, 1114, 4912, 4789, 3837,
        2584, 4547, 2717, 1979, 1515, 5082, 2123, 2702, 1383, 4852, 2103, 1631,
        3928, 1047,  644, 4599, 4089, 3237, 1322, 3410, 3510, 1789,  356, 4829,
        2528, 4193, 1380, 3242, 2344, 2097,  689, 1329,  418, 1659, 4076, 3367,
        3690,   44, 3721, 2756, 1259,  943, 1016, 4637, 5388, 5024, 1894, 4279,
        3005, 4470, 2654, 3234, 4316,  549, 2935, 2012, 3310, 5262, 2154, 2154,
        4334, 3546, 2159, 3777, 2213,  738, 2459, 1825, 1190, 5088, 3781, 3281,
        4102, 2961, 3838, 5113, 4734,  904, 1429,  378, 5367, 1288, 3627,  473,
        1152, 2754,  574, 2133, 2940, 1850, 3055, 2934, 2142, 3816, 3722, 3768,
        3084, 1950, 3357, 5140, 3271, 5372, 2408, 4065,  198, 4197, 2836,   28,
        1523, 2923, 4214,  280, 3019, 4389, 3569, 5118], device='cuda:0')
> /home/jeongtae/RLMPPI/src/algorithm/helper.py(262)sample()
-> next_s[t] = self._get_s(self._s, _idxs+1)
tensor([5000, 4000, 3464,  678, 3000,    0,    0, 4000,  821, 3514, 3106, 3000,
        2500, 3284, 5000, 4500, 3179,    0,  500,    0,  500, 1500, 2500,  587,
        3412,  500, 2000,  500, 4000, 4000, 2500, 1500, 4500, 2000, 4500, 3000,
        3000,  500, 4000, 2000, 1500, 2500, 2500, 3000,    0, 2003, 4500, 3038,
        5000, 2000, 1500,    0,  500,    0, 3057, 2500,    0,  500, 3000,  500,
        3135,  972, 3451,    0, 5000, 2500, 4000, 4500, 3000, 3000,  816, 2500,
         770, 4000, 3000, 2500, 5000, 3000, 2500, 1500,  500, 2500,  500, 2000,
        3000,    0,  597, 4000, 3000, 4500,  778,  500, 2500, 4500, 1500, 4500,
        2000, 1500,  500, 2500, 2500, 4000,  500,  825,  500,  620, 4000, 2500,
        2000, 3065,    0, 5000, 4000,    0,  500,  500, 3000, 2000,    0, 3000,
        3000, 5000, 2500, 5000, 1500, 1505, 2000, 3011, 1500, 4000,  773,  516,
        5000, 1500, 2500, 3137, 3000,  500, 5000,    0, 4500, 2000, 3000, 2000,
        3048,    0, 2000, 4000, 2000, 4500, 2000, 5000, 4500, 4500, 3000, 1500,
         500, 2000,  500, 4000, 4000, 2500, 4500, 3000,  500,  731, 1500, 2000,
         500, 3315, 3000, 3380, 2500, 5000, 2500,  500, 5000, 4000, 5000,    0,
         500, 1500,  500, 2500, 5000,  500, 3000,  682, 4500, 5000,  500, 3000,
        1500,    0,    0, 3361,    0, 2000, 5000, 2500, 1500,    0, 4000,  500,
        5000, 4000, 3314, 4500, 1500, 3290, 4500,  500,  709, 5000, 3384, 3000,
        1500, 2000, 5000, 2500,  500, 5000, 1500, 5000, 5000, 3000, 3279, 3000,
        3419, 5000, 2000, 5000, 3117,  627, 1500, 3266, 1500,  713,  874, 5000,
        3299,    0, 2000, 3264, 3282, 3370,  500, 4500,    0, 3000, 1500, 4500,
         500, 2000,    0, 3226, 5000, 3312, 5000, 5000, 2000,  669, 4000, 2500,
         689, 3234,  500, 1500, 4500,  500, 2000,  500,    0, 3193, 2000,    0,
        3000, 2500, 3223, 1500, 2000, 2500, 1500, 3206, 4000,  500,    0, 4500,
        4500, 4500,  500, 3000, 2500, 4000, 2500, 4000, 4000, 4000, 3386,  817,
        4500, 4500, 5000, 1500,  952, 2500, 2000, 4500, 3359, 4500, 2500,  545,
         627, 2500, 1500, 5000, 3286, 4000, 3000,  500, 1500, 3000, 1500,  500,
        4500, 4000, 2000, 5000, 2500, 5000, 4000, 3187, 1500, 3000, 5000, 2000,
        5000, 5000, 2000, 4000, 5000,  500, 5000, 1500, 5000, 4000, 2000,  819,
        3474, 5000, 5000, 3437,    0, 3234, 1500, 2500, 2000,    0, 2500,    0,
        3000,  929,  500, 2500,  671, 3470, 2500,    0,    0,  807, 2500, 3000,
        3081, 4500, 3000, 2500, 1500,    0,  500, 5000, 1500, 4000, 2000,    0,
         668, 1500, 5000, 3200, 2000,    4, 1500,  787, 1500, 5000, 5000, 4000,
        3000, 5000, 3000, 2000, 2000,    0, 2500, 3000, 1500, 5000, 2500, 2000,
        4000, 1500,  644, 5000, 4500, 3237, 1500, 3410, 3510, 2000,  500, 5000,
        3000, 4500, 1500, 3242, 2500, 2500,  689, 1500,  500, 2000, 4500, 3367,
        4000,  500, 4000, 3000, 1500,  943, 1500, 5000,    0,    0, 2000, 4500,
        3005, 4500, 3000, 3234, 4500,  549, 3000, 2500, 3310,    0, 2500, 2500,
        4500, 4000, 2500, 4000, 2500,  738, 2500, 2000, 1500,    0, 4000, 3281,
        4500, 3000, 4000,    0, 5000,  904, 1500,  500,    0, 1500, 4000,  500,
        1500, 3000,  574, 2500, 3000, 2000, 3055, 3000, 2500, 4000, 4000, 4000,
        3084, 2000, 3357,    0, 3271,    0, 2500, 4500,  500, 4500, 3000,  500,
        2000, 3000, 4500,  500, 3019, 4500, 4000,    0], device='cuda:0')
Traceback (most recent call last):
  File "train.py", line 109, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 77, in train
    train_metrics.update(agent.update(buffer,step+i))
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 174, in update
    s, next_ses, action, reward, idxs, weights = replay_buffer.sample() #버퍼들중 무작위 값 불러와서 값들 초기화
  File "/home/jeongtae/RLMPPI/src/algorithm/helper.py", line 262, in sample
    next_s[t] = self._get_s(self._s, _idxs+1)
  File "/home/jeongtae/RLMPPI/src/algorithm/helper.py", line 262, in sample
    next_s[t] = self._get_s(self._s, _idxs+1)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit