[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 3429.4           [30mT:[39m 0:00:00
Traceback (most recent call last):
  File "train.py", line 105, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 90, in train
    common_metrics['episode_reward'] = evaluate(env, agent, cfg.eval_episodes, step, eval_numbering)
  File "train.py", line 35, in evaluate
    s, reward, done, _ = env.step(action.cpu().numpy())
  File "/home/jeongtae/RLMPPI/src/environment.py", line 91, in step
    reward,done =self.map.calculate_reward(self.state)
  File "/home/jeongtae/RLMPPI/src/environment.py", line 66, in calculate_reward
    Cost = torch.exp(-(((x.view(-1, 1) - obs_x)**2 / sigma_x**2) + ((y.view(-1, 1) - obs_y)**2 / sigma_y**2)))
KeyboardInterrupt