[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m 3429.4           [30mT:[39m 0:00:00
> /home/jeongtae/RLMPPI/src/train.py(33)evaluate()
-> while not done:
[array([7.67597576e-05, 6.54529720e-05, 6.98131701e-01, 1.00876915e-03]), array([7.67597576e-05, 6.54529720e-05, 6.98131701e-01, 1.00876915e-03])]
> /home/jeongtae/RLMPPI/src/train.py(34)evaluate()
-> action = agent.plan(s, eval_mode=True, step=step, t0=t==0)
Traceback (most recent call last):
  File "train.py", line 106, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 91, in train
    common_metrics['episode_reward'] = evaluate(env, agent, cfg.eval_episodes, step, eval_numbering)
  File "train.py", line 34, in evaluate
    action = agent.plan(s, eval_mode=True, step=step, t0=t==0)
  File "train.py", line 34, in evaluate
    action = agent.plan(s, eval_mode=True, step=step, t0=t==0)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit