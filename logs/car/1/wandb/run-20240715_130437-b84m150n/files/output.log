[34m[1mLogs will be synced with wandb.
 [34mtrain[39m   [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m -2085.7          [30mT:[39m 0:00:00
 [32meval[39m    [30mE:[39m 1                [30mS:[39m 0                [30mR:[39m -1620.2          [30mT:[39m 0:00:00
 [34mtrain[39m   [30mE:[39m 2                [30mS:[39m 1,000            [30mR:[39m -2106.7          [30mT:[39m 0:00:40
 [34mtrain[39m   [30mE:[39m 3                [30mS:[39m 2,000            [30mR:[39m -2469.9          [30mT:[39m 0:00:40
 [34mtrain[39m   [30mE:[39m 4                [30mS:[39m 3,000            [30mR:[39m -241.3           [30mT:[39m 0:00:40
 [34mtrain[39m   [30mE:[39m 5                [30mS:[39m 4,000            [30mR:[39m 106.7            [30mT:[39m 0:00:40
 [34mtrain[39m   [30mE:[39m 6                [30mS:[39m 5,000            [30mR:[39m -788.5           [30mT:[39m 0:00:41
 [34mtrain[39m   [30mE:[39m 7                [30mS:[39m 6,000            [30mR:[39m -1858.8          [30mT:[39m 0:00:41
 [34mtrain[39m   [30mE:[39m 8                [30mS:[39m 7,000            [30mR:[39m -2485.0          [30mT:[39m 0:00:41
 [34mtrain[39m   [30mE:[39m 9                [30mS:[39m 8,000            [30mR:[39m -2394.1          [30mT:[39m 0:00:41
 [34mtrain[39m   [30mE:[39m 10               [30mS:[39m 9,000            [30mR:[39m 309.9            [30mT:[39m 0:00:41
Traceback (most recent call last):
  File "train.py", line 109, in <module>
    train(parse_cfg(Path().cwd() / __CONFIG__))
  File "train.py", line 77, in train
    train_metrics.update(agent.update(buffer,step+i))
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 190, in update
    td_target = self._td_target(next_ses[t], reward[t]) #Îã§Ïùå stepÏùò td targetÏùÑ Í≥ÑÏÇ∞Ìï®
  File "/home/jeongtae/anaconda3/envs/RLMPPI/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 170, in _td_target
    torch.min(*self.model_target.Q(next_ss, self.model.pi(next_ss, self.cfg.min_std))) #reward+Í∞êÍ∞ÄÏú®*value(target modelÏóêÏÑú encoding Îêú obsÏôÄ action)
  File "/home/jeongtae/RLMPPI/src/algorithm/tdmpc.py", line 37, in pi
    return h.TruncatedNormal(mu, std).sample(clip=0.3)
  File "/home/jeongtae/RLMPPI/src/algorithm/helper.py", line 77, in sample
    return self._clamp(x)
  File "/home/jeongtae/RLMPPI/src/algorithm/helper.py", line 64, in _clamp
    clamped_x = torch.clamp(x, self.low + self.eps, self.high - self.eps)
KeyboardInterrupt